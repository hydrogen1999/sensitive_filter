{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from train.model.model import SensitiveClassifier\n",
        "from ultils import *\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "#Load tokenizer\n",
        "with open('D:/AI Project/sensitive_filter/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "    \n",
        "model = SensitiveClassifier(n_classes=2)\n",
        "model.load_state_dict(torch.load('./checkpoints/phobert_fold3.pth', map_location=torch.device(device)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_input_and_baseline(text, tokenizer):\n",
        "    max_length = 510\n",
        "    baseline_token_id = tokenizer.pad_token_id \n",
        "    sep_token_id = tokenizer.sep_token_id \n",
        "    cls_token_id = tokenizer.cls_token_id \n",
        "\n",
        "    text_ids = tokenizer.encode(text, max_length=max_length, truncation=True, add_special_tokens=False)\n",
        "   \n",
        "    input_ids = [cls_token_id] + text_ids + [sep_token_id]\n",
        "    token_list = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "  \n",
        "\n",
        "    baseline_input_ids = [cls_token_id] + [baseline_token_id] * len(text_ids) + [sep_token_id]\n",
        "    return torch.tensor([input_ids], device='cpu'), torch.tensor([baseline_input_ids], device='cpu'), token_list\n",
        "\n",
        "def summarize_attributions(attributions):\n",
        "\n",
        "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
        "    attributions = attributions / torch.norm(attributions)\n",
        "    \n",
        "    return attributions\n",
        "\n",
        "def interpret_text(text, model, tokenizer, true_class = 'unknow'):\n",
        "\n",
        "    input_ids, baseline_input_ids, all_tokens = construct_input_and_baseline(text, tokenizer)\n",
        "# Define model output\n",
        "    def model_output(inputs):\n",
        "        return model(inputs)[0]\n",
        "\n",
        "    # Define model input\n",
        "    model_input = model.bert.embeddings\n",
        "    lig = LayerIntegratedGradients(model_output, model_input)\n",
        "    attributions, delta = lig.attribute(inputs= input_ids,\n",
        "                                    baselines= baseline_input_ids,\n",
        "                                    return_convergence_delta=True,\n",
        "                                    internal_batch_size=1\n",
        "                                    )\n",
        "    attributions_sum = summarize_attributions(attributions)\n",
        "    score_vis = viz.VisualizationDataRecord(\n",
        "                        word_attributions = attributions_sum,\n",
        "                        pred_prob = torch.max(model(input_ids)[0]),\n",
        "                        pred_class = torch.argmax(model(input_ids)[0]).numpy(),\n",
        "                        true_class = true_class,\n",
        "                        attr_class = text,\n",
        "                        attr_score = attributions_sum.sum(),       \n",
        "                        raw_input_ids = all_tokens,\n",
        "                        convergence_score = delta)\n",
        "    viz.visualize_text([score_vis])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>unknow</b></text></td><td><text style=\"padding-right:2em\"><b>1 (2.46)</b></text></td><td><text style=\"padding-right:2em\"><b>ngu thì mất tiền thôi than lên đây làm gì</b></text></td><td><text style=\"padding-right:2em\"><b>-0.38</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ngu                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> thì                    </font></mark><mark style=\"background-color: hsl(0, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mất                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tiền                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> thôi                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> than                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lên                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> đây                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> làm                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> gì                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark></td><tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>unknow</b></text></td><td><text style=\"padding-right:2em\"><b>0 (3.59)</b></text></td><td><text style=\"padding-right:2em\"><b>chứng khoán dạo này chán lắm</b></text></td><td><text style=\"padding-right:2em\"><b>1.38</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> chứng                    </font></mark><mark style=\"background-color: hsl(120, 75%, 61%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> khoán                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dạo                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> này                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> chán                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lắm                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark></td><tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "text = \"ngu thì mất tiền thôi than lên đây làm gì\"\n",
        "true_class = 1\n",
        "interpret_text(text, model, tokenizer)\n",
        "text = \"chứng khoán dạo này chán lắm, tất tay giờ là toang\"\n",
        "true_class = 0\n",
        "interpret_text(text, model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "0250446e861f3e3bb58ab720aa611cbf2b8f13eae5d18b0ae3b7b996675fb1df"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
